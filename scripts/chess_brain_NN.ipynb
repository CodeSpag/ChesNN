{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The brain\n",
    "\n",
    "## Creating the database\n",
    "This is where I train the neural network to handle evaluation predictions. The idea behind this engine is to learn to predict how Stockfish would evaluate a position. Lichess kindly provides us with a database of 21M positions with stockfish's evaluation in a JSON format. We'll create a sqlite database as storage and interact with it using pandas.\n",
    "### Compiling the data\n",
    "From a given position, Stockfish is looking at several lines into the future and determining the current position's eval based on that information.\n",
    "To keep it manageable, I will average the eval of the lines that Stockfish looked into and used that result as the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import chess\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database\n",
    "with sqlite3.connect('../assets/data/evaluations_avg.sqlite') as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create table to store evaluations (we don't need depth, knodes or lines)\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS evaluations_avg\n",
    "                  (fen TEXT PRIMARY KEY, average_cp REAL)''')\n",
    "\n",
    "    # Read JSONL file and insert data into the SQLite database\n",
    "    with open('../assets/data/lichess_db_eval.jsonl', 'r') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "                data = json.loads(line.strip())  # Load JSON object from each line\n",
    "                fen = data[\"fen\"]\n",
    "                \n",
    "                # looking at all the evals and averaging their cp\n",
    "                total_cp = 0\n",
    "                num_eval = 0\n",
    "                for eval_data in data[\"evals\"]:\n",
    "                    for pv in eval_data[\"pvs\"]:\n",
    "                        try: # some evals don't have \"cp\" because mate is present. \n",
    "                            cp = pv[\"cp\"]\n",
    "                        except:\n",
    "                            cp = pv[\"mate\"] # treating mate just like cp; if mate is present, it just means the position is extremely winning for one side\n",
    "                        cp = max(-15, min(cp, 15))  # Clamp the value of \"cp\" between -15 and 15\n",
    "                        total_cp += cp\n",
    "                        num_eval += 1\n",
    "                        \n",
    "                if num_eval > 0: #guard clause to avoid division by zero errors\n",
    "                    average_cp = total_cp / num_eval\n",
    "                else:\n",
    "                    average_cp = 0\n",
    "\n",
    "                # Insert average cp for the position into the database\n",
    "                cursor.execute('''INSERT INTO evaluations_avg (fen, average_cp)\n",
    "                                VALUES (?, ?)''', (fen, average_cp))\n",
    "\n",
    "    # Commit changes and close connection\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new connection to avoid rerunning the cell above\n",
    "conn_ = sqlite3.connect('../assets/data/evaluations_avg.sqlite')\n",
    "df_sample = pd.read_sql(\"\"\" SELECT *\n",
    "                    FROM evaluations_avg\n",
    "                    LIMIT 10000\n",
    "                 \"\"\", conn_)\n",
    "\n",
    "df_sample.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "I was hoping that we would get an evaluation that's closer to the truth: the first position in the df is the starting position\n",
    "and it's evaluated at +3.11. In reality, it should evaluate to 0.5. Inspecting the data, it's far off the actual eval when I paste\n",
    "the FEN in an online engine.\n",
    "\n",
    "My hope is that with this large dataset to train on, we can make sensible decisions regardless of the fact that a portion of the data is off.\n",
    "While +3 is wrong for the starting position, it is true that it's generally better for white. I'm curious to see if that is enough to make a sensible decision on which move to make.\n",
    "\n",
    "We'll call this V1. If it doesn't work, there is always a database of games available on lichess that contains the exact evaluation from Stockfish.\n",
    "\n",
    "## Translating FEN notation to neuron-speak\n",
    "There is a precise way to achieve this in chess computing and that's with the use of BitBoards. These are 64-bits numbers that represent the presence or absence of a given piece. As there are 12 different pieces (6 for each colors), we need at least 12 bitboards to represent a complete board state. One approach would be to feed a (8,8,12) array to the neural network, but it will flatten it into 1D array anyway to read it and lose some spatial information. Another approach would be to leverage what other clever people have figured out; the direct translation of 12 bitboards together to an integer. Since the spatial information is going to get lost anyway, might as well try this one out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the chess module has a function that returns an int representation of the bitboards for a given position\n",
    "\n",
    "board = chess.Board(chess.STARTING_FEN)\n",
    "board.occupied\n",
    "\n",
    "# I am not convinced that this is the correct integer representation, but as long as it's consistent, the\n",
    "# model will learn properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the entire db into memory. Luckily it takes only 320 mb\n",
    "\n",
    "df_full = pd.read_sql(\"\"\" SELECT *\n",
    "                    FROM evaluations_avg\n",
    "                        \"\"\", conn_)\n",
    "df_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column with the int representation of the FEN position\n",
    "\n",
    "def fen_to_bitboard_int(fen: str) -> int:\n",
    "    \"\"\" return an int representation of the fen, returns None if there's an error\"\"\"\n",
    "    try:\n",
    "        return chess.Board(fen).occupied\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "df_full[\"bitboard_int\"] = df_full[\"fen\"].apply(fen_to_bitboard_int)\n",
    "df_full.dropna(inplace=True) # drop some errors\n",
    "df_full.drop(columns=\"fen\", inplace=True) # we don't need the fen anymore\n",
    "\n",
    "# Save the DataFrame as a new SQLite database\n",
    "temp_conn = sqlite3.connect('../assets/data/train_data_v1.sqlite')\n",
    "df_full.to_sql('train_data_v1', temp_conn, if_exists='replace', index=False)\n",
    "temp_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "I'm doing this with Tensorflow using Keras interface. I want to go up to 64 neurons, 1 for every square. It's only looking at 1 feature and has 1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the training df from sqlite\u001b[39;00m\n\u001b[0;32m      2\u001b[0m temp_conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../assets/data/train_data_v1.sqlite\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m SELECT *\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m                    FROM train_data_v1\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m                        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m, temp_conn)\n\u001b[0;32m      6\u001b[0m temp_conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m df_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitboard_int\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Pix\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:654\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 654\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    655\u001b[0m             sql,\n\u001b[0;32m    656\u001b[0m             index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    657\u001b[0m             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    658\u001b[0m             coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    659\u001b[0m             parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    660\u001b[0m             chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    661\u001b[0m             dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    662\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    663\u001b[0m         )\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    666\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32mc:\\Users\\Pix\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2341\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_iterator(\n\u001b[0;32m   2331\u001b[0m         cursor,\n\u001b[0;32m   2332\u001b[0m         chunksize,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2338\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   2339\u001b[0m     )\n\u001b[0;32m   2340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2341\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetchall_as_list(cursor)\n\u001b[0;32m   2342\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   2344\u001b[0m     frame \u001b[38;5;241m=\u001b[39m _wrap_result(\n\u001b[0;32m   2345\u001b[0m         data,\n\u001b[0;32m   2346\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2351\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   2352\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Pix\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2356\u001b[0m, in \u001b[0;36mSQLiteDatabase._fetchall_as_list\u001b[1;34m(self, cur)\u001b[0m\n\u001b[0;32m   2355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetchall_as_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, cur):\n\u001b[1;32m-> 2356\u001b[0m     result \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[0;32m   2357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   2358\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(result)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load the training df from sqlite\n",
    "temp_conn = sqlite3.connect('../assets/data/train_data_v1.sqlite')\n",
    "df_train = pd.read_sql(\"\"\" SELECT *\n",
    "                    FROM train_data_v1\n",
    "                        \"\"\", temp_conn)\n",
    "temp_conn.close()\n",
    "X = df_train[\"bitboard_int\"]\n",
    "y = df_train[\"average_cp\"]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scaling between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X = np.array(X)\n",
    "X = X.reshape(-1, 1)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "model1 = keras.Sequential(\n",
    "    [\n",
    "        # Input layer -> 1 integer\n",
    "        keras.Input(shape=(1,)),\n",
    "        # Hidden layers\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64), #inserting 1 batch norm\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        # output layer -> 1 raw integer (prediction)\n",
    "        layers.Dense(1, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model1.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", metrics=[\"root_mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first training session, batch size of 100 and 1 epoch seems like a good start\n",
    "model1.fit(X, y, batch_size=50, epochs=1, validation_split=0.1)\n",
    "\n",
    "# did 2 more epochs, it's not learning anymore, I'll run 200 batch size and see if it improves\n",
    "# after 3 epochs of 200 batch size, it's barely improving. increasing to 500 batch size.\n",
    "# 3 more epochs, it's actually getting worse. Let's try a different architecture and start over\n",
    "\n",
    "# doing 1, 32, 64, 64, 64, 32, 1 and running 3 epochs with a starting batch size of 10. EDIT: it takes 10 min/epoch, ran only 1\n",
    "# it's already better with 1 epoch. I'll increase the batch size to 50 so I don't die of old age.\n",
    "# added a batchnormalization to try to improve accuracy\n",
    "# scaled the data between 0 and 1, immediate increase!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prediction(fen:str) -> int:\n",
    "    \"\"\"Takes a fen a returns a prediction int made by the model\"\"\"\n",
    "    x = fen_to_bitboard_int(fen)\n",
    "    # Reshape the array to have shape (1,)\n",
    "    x_array = np.array([x])\n",
    "    x_reshaped = x_array.reshape(1, -1)\n",
    "    x_transformed = scaler.transform(x_reshaped)\n",
    "    return model1.predict(x_transformed)[0][0]\n",
    "\n",
    "print(get_prediction(\"rnbqr1k1/ppp2ppp/8/4N3/1P1bQ3/P7/5PPP/R1B1KB1R w KQ - 0 13\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the way I translate the FEN\n",
    "To no one's surprise, the approach of transforming the position to an integer makes it lose a lot of its meaning, and the result integer is so enormous that it needs to get scaled back down, which blurs it even further.\n",
    "On a range of -15 to 15, the model was getting at best an average error of 13.3. Horrible.\n",
    "\n",
    "Instead, I will compile all 12 bitboards into a single array to represent the piece positions and add a few numbers for game state details. Hopefully this will help the model paint a clearer picture of what's going on.\n",
    "\n",
    "I am 80% sure that it will be necessary to retrain the model using the games database instead because they contain the accurate Stockfish evaluation instead of this averaging that I came up with. I don't want to this just yet though because that db is HUGE and I'm still looking for the best recipe to train this NN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16,   4,   0,  32,   0, 128,   4,   2,   0,  64,   1,   0, 128,\n",
       "         8,   0, 128,   8,   0, 128,   8,   0, 128,   8,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   2,   0,  32,   2,   0,  32,\n",
       "         2,   0,  32,   2,   0,  32,   0,  64,  16,   0, 128,   2,   0,\n",
       "        16,   8,   1,   0,   4, 248], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fen2bitboard(fen: str, to_bits: bool=False) -> np.array:\n",
    "    \"\"\"\n",
    "    Returns bitboard [np 1D array(773)] from fen\n",
    "    \"\"\"\n",
    "    # each square is assigned 12 bits to represent each piece, that's what the mapping is for\n",
    "    mapping = {\n",
    "                \"p\": 0,\n",
    "                \"n\": 1,\n",
    "                \"b\": 2,\n",
    "                \"r\": 3,\n",
    "                \"q\": 4,\n",
    "                \"k\": 5,\n",
    "                \"P\": 6,\n",
    "                \"N\": 7,\n",
    "                \"B\": 8,\n",
    "                \"R\": 9,\n",
    "                \"Q\": 10,\n",
    "                \"K\": 11\n",
    "                }\n",
    "    \n",
    "    # initialize the array with zeros\n",
    "    bitboard = np.zeros(773, dtype=int)\n",
    "    currIndex = 0\n",
    "    \n",
    "    try:\n",
    "        position, turn, castling, _, _, _ = fen.split(\" \") # keep only useful data\n",
    "    except:\n",
    "        position, turn, castling, _ = fen.split(\" \")\n",
    "    \n",
    "    for ch in position:\n",
    "        if ch == \"/\": # \"/\" represent rows, simply ignore that\n",
    "            continue\n",
    "        elif ch.isdigit(): # a digit means an empty space, skip ahead that many indexes\n",
    "            currIndex += int(ch) * 12 # multiply by 12 because there are 12 bits used for each square\n",
    "        else:\n",
    "            bitboard[currIndex + mapping[ch]] = 1 # set the correct bit to 1\n",
    "            currIndex += 12 # get to next bit\n",
    "    \n",
    "    # add details about the game state\n",
    "    bitboard[768] = 1 if turn == \"w\" else 0\n",
    "    bitboard[769] = 1 if \"K\" in castling else 0\n",
    "    bitboard[770] = 1 if \"Q\" in castling else 0\n",
    "    bitboard[771] = 1 if \"k\" in castling else 0\n",
    "    bitboard[772] = 1 if \"q\" in castling else 0\n",
    "    \n",
    "    if to_bits:\n",
    "        return np.packbits(bitboard)\n",
    "    return bitboard\n",
    "\n",
    "fen2bitboard(chess.STARTING_FEN, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_cp</th>\n",
       "      <th>bitboard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.111111</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.545455</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.222222</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.913043</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.666667</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.400000</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.571429</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.444444</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-15.000000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-15.000000</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-9.000000</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.777778</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-15.000000</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-5.000000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    average_cp                                           bitboard\n",
       "0     3.111111  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "1    15.000000  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "2     9.545455  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3    -0.222222  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "4    15.000000  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "5    14.913043  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "6     1.666667  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "7    -1.400000  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "8     9.571429  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "9     0.000000  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "10    7.444444  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "11    0.000000  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "12  -15.000000  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "13  -15.000000  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "14   -9.000000  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "15    3.777778  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "16  -15.000000  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "17   -5.000000  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "18   15.000000  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "19   15.000000  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have a working bitboard maker, lets rewrite the training data\n",
    "conn_ = sqlite3.connect('../assets/data/evaluations_avg.sqlite')\n",
    "df_full = pd.read_sql(\"\"\" SELECT *\n",
    "                    FROM evaluations_avg\n",
    "                        \"\"\", conn_)\n",
    "conn_.close()\n",
    "df_full[\"bitboard\"] = df_full[\"fen\"].apply(fen2bitboard)\n",
    "df_full.drop(columns=\"fen\", inplace=True)\n",
    "df_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_full\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_full' is not defined"
     ]
    }
   ],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_conn = sqlite3.connect('../assets/data/train_data_v2.sqlite')\n",
    "df_full.to_sql('train_data_v2', temp_conn, if_exists='replace', index=False)\n",
    "temp_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(773,), dtype=tf.int16, name=None),\n",
       " TensorSpec(shape=(1,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have a problem, this new database doesn't fit into memory.\n",
    "# trying a generator\n",
    "import tensorflow as tf\n",
    "\n",
    "# open a connection\n",
    "conn = sqlite3.connect('../assets/data/evaluations_avg.sqlite')\n",
    "\n",
    "def data_generator():\n",
    "        \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM evaluations_avg\")\n",
    "    batch_size = 1000\n",
    "    while True:\n",
    "        rows = cursor.fetchmany(batch_size)\n",
    "        if not rows:\n",
    "            break\n",
    "        X_batch = [fen2bitboard(row[0]) for row in rows]  # Feature column\n",
    "        y_batch = [row[1] for row in rows]  # Label column\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "# # close when done\n",
    "# conn.close()\n",
    "# cursor.close()\n",
    "\n",
    "\n",
    "# create a tensor dataset\n",
    "dataset = tf.data.Dataset.from_generator(generator = data_generator,\n",
    "                                        output_signature= (\n",
    "                                        tf.TensorSpec(shape=(773,), dtype=tf.int16), # bitboard\n",
    "                                        tf.TensorSpec(shape=(1), dtype=tf.float32)  # average_cp\n",
    "                                    )\n",
    "                                    )\n",
    "\n",
    "\n",
    "dataset.element_spec\n",
    "# it doesn't work with SQL query, I'm running into graph errors and asks me to use only 1 thread for the sql queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">350</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">270,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">63,180</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,290</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">360</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,824</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m350\u001b[0m)            │       \u001b[38;5;34m270,900\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │        \u001b[38;5;34m63,180\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m)             │        \u001b[38;5;34m16,290\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m)             │           \u001b[38;5;34m360\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m5,824\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">358,667</span> (1.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m358,667\u001b[0m (1.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">358,487</span> (1.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m358,487\u001b[0m (1.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">180</span> (720.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m180\u001b[0m (720.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's adjust our model for this\n",
    "\n",
    "model2 = keras.Sequential(\n",
    "    [\n",
    "        # Input layer -> 1 integer\n",
    "        keras.Input(shape=(773,)),\n",
    "        # Hidden layers\n",
    "        layers.Dense(350, activation=\"relu\"),\n",
    "        layers.Dense(180, activation=\"relu\"),\n",
    "        layers.Dense(90), #inserting 1 batch norm\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        # output layer -> 1 raw integer (prediction)\n",
    "        layers.Dense(1, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model2.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", metrics=[\"root_mean_squared_error\"])\n",
    "model2.summary()\n",
    "\n",
    "\n",
    "# making train and test data\n",
    "test_dataset = dataset.take(10000) # takes the first 10000 of the data for\n",
    "train_dataset = dataset.skip(10000) # takes the remaining for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m209893/209893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 2ms/step - loss: 4.8852 - root_mean_squared_error: 8.5216 - val_loss: 6.6086 - val_root_mean_squared_error: 10.0279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23000ea0590>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model2.fit(train_dataset.batch(100), epochs=1, validation_data=test_dataset.batch(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"first_model_keep_training.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found out tensorflow has a built-in method for sql db...\n",
    "\n",
    "dataset = tf.data.experimental.SqlDataset(\"sqlite\", \"../assets/data/train_data_v2.sqlite\", \n",
    "                                          \"SELECT average_cp, bitboard FROM train_data_v2\", \n",
    "                                          (tf.float64, tf.string))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let her rip\n",
    "X = df_full[\"bitboard\"].apply(lambda x: tf.convert_to_tensor(x))\n",
    "y = df_full[\"average_cp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2.fit(X, y, batch_size=50, epochs=3, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a connection\n",
    "conn = sqlite3.connect('../assets/data/evaluations_avg.sqlite')\n",
    "\n",
    "# Fetch the number of rows in the table\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT COUNT(*) FROM evaluations_avg\")\n",
    "num_rows = cursor.fetchone()[0]\n",
    "cursor.close()\n",
    "\n",
    "# Initialize empty arrays\n",
    "X_data = np.empty((num_rows, 773), dtype=np.int16)\n",
    "y_data = np.empty((num_rows,), dtype=np.float32)\n",
    "\n",
    "# Fetch data from the SQLite database in batches\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM evaluations_avg\")\n",
    "batch_size = 1000\n",
    "start_idx = 0\n",
    "while True:\n",
    "    rows = cursor.fetchmany(batch_size)\n",
    "    if not rows:\n",
    "        break\n",
    "    for row in rows:\n",
    "        X_data[start_idx] = fen2bitboard(row[0])\n",
    "        y_data[start_idx] = row[1]\n",
    "        start_idx += 1\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(773,), dtype=tf.int16, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create TensorFlow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completely crashes kernel... even if it separates the dataset into shard. idk why\n",
    "dataset.save(\"../assets/data/train_dataset_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.load_model(\"first_model_keep_training.keras\")\n",
    "\n",
    "# making train and test data\n",
    "test_dataset = dataset.take(10000) # takes the first 10000 of the data for\n",
    "train_dataset = dataset.skip(10000) # takes the remaining for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1395s\u001b[0m 1ms/step - loss: 4.5089 - root_mean_squared_error: 8.0830 - val_loss: 5.6494 - val_root_mean_squared_error: 8.9893\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1400s\u001b[0m 1ms/step - loss: 4.4555 - root_mean_squared_error: 8.0211 - val_loss: 5.8084 - val_root_mean_squared_error: 9.0748\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1422s\u001b[0m 1ms/step - loss: 4.4323 - root_mean_squared_error: 7.9969 - val_loss: 5.6499 - val_root_mean_squared_error: 8.9295\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1429s\u001b[0m 1ms/step - loss: 4.4154 - root_mean_squared_error: 7.9812 - val_loss: 5.6932 - val_root_mean_squared_error: 9.0842\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1449s\u001b[0m 1ms/step - loss: 4.3997 - root_mean_squared_error: 7.9620 - val_loss: 5.7224 - val_root_mean_squared_error: 9.1301\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1461s\u001b[0m 1ms/step - loss: 4.3882 - root_mean_squared_error: 7.9519 - val_loss: 5.6152 - val_root_mean_squared_error: 8.9583\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1468s\u001b[0m 1ms/step - loss: 4.3747 - root_mean_squared_error: 7.9345 - val_loss: 5.5796 - val_root_mean_squared_error: 8.8829\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1472s\u001b[0m 1ms/step - loss: 4.3657 - root_mean_squared_error: 7.9256 - val_loss: 5.6294 - val_root_mean_squared_error: 8.9222\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1499s\u001b[0m 1ms/step - loss: 4.3558 - root_mean_squared_error: 7.9161 - val_loss: 5.6583 - val_root_mean_squared_error: 9.0238\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1491s\u001b[0m 1ms/step - loss: 4.3462 - root_mean_squared_error: 7.9070 - val_loss: 5.5950 - val_root_mean_squared_error: 8.8748\n"
     ]
    }
   ],
   "source": [
    "# save after every epoch to make sure we don't lose progress\n",
    "for _ in range(10):\n",
    "    model2.fit(train_dataset.batch(20), epochs=1, validation_data=test_dataset.batch(100))\n",
    "    model2.save(\"first_model_keep_training.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_1_1/Cast:0\", shape=(32,), dtype=float32). Expected shape (None, 773), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32,), dtype=int32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#test_dataset.save(\"../assets/data/test_dataset\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model2 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_model_keep_training.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model2\u001b[38;5;241m.\u001b[39mpredict(fen2bitboard(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr1b1k1nr/ppp2ppp/8/2b1P3/4p3/2P5/PP3PPP/RNBK1B1R w kq - 1 8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Pix\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Pix\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:280\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    278\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    279\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"sequential_1_1/Cast:0\", shape=(32,), dtype=float32). Expected shape (None, 773), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32,), dtype=int32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "#test_dataset.save(\"../assets/data/test_dataset\")\n",
    "model2 = keras.models.load_model(\"first_model_keep_training.keras\")\n",
    "model2.predict(fen2bitboard(\"r1b1k1nr/ppp2ppp/8/2b1P3/4p3/2P5/PP3PPP/RNBK1B1R w kq - 1 8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14.912656]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model needs to be loaded before hand\n",
    "def get_pred(fen: str) -> np.array:\n",
    "    pos = fen2bitboard(fen)\n",
    "    pos = np.reshape(pos, (1, 773)) # reshaping because model expects a dimension for the batch size\n",
    "    return model2.predict(pos)\n",
    "\n",
    "get_pred(\"r1b1k1nr/pppp2pp/4pp2/8/7q/2NB4/PPP3PP/R1BQ1RK1 w - - 0 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cursor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# DON'T FORGET TO CLOSE CONN WHEN DONE TRAINING\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m cursor\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m      4\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cursor' is not defined"
     ]
    }
   ],
   "source": [
    "# DON'T FORGET TO CLOSE CONN WHEN DONE TRAINING\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
