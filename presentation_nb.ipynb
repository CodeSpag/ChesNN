{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network to play chess\n",
    "The purpose of this project is to train a neural network and integrate it into a fully fonctionning program. I am not trying to make a good chess engine; this project is intended to hone my data science and machine learning skills and serve as a portfolio showcase.\n",
    "\n",
    "## Creating the database\n",
    "This is where I train the neural network to handle evaluation predictions. The idea behind this engine is to learn to predict how Stockfish would evaluate a position. Lichess kindly provides us with a database of 21M positions with stockfish's evaluation in a JSON format. We'll create a sqlite database as storage and interact with it using pandas.\n",
    "### Compiling the data\n",
    "From a given position, Stockfish is looking at several lines into the future and determining the current position's eval based on that information.\n",
    "To keep it manageable, I will average the eval of the lines that Stockfish looked into and used that result as the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import chess\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database\n",
    "with sqlite3.connect('../assets/data/evaluations_avg.sqlite') as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create table to store evaluations (we don't need depth, knodes or lines)\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS evaluations_avg\n",
    "                  (fen TEXT PRIMARY KEY, average_cp REAL)''')\n",
    "\n",
    "    # Read JSONL file and insert data into the SQLite database\n",
    "    with open('../assets/data/lichess_db_eval.jsonl', 'r') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "                data = json.loads(line.strip())  # Load JSON object from each line\n",
    "                fen = data[\"fen\"]\n",
    "                \n",
    "                # looking at all the evals and averaging their cp\n",
    "                total_cp = 0\n",
    "                num_eval = 0\n",
    "                for eval_data in data[\"evals\"]:\n",
    "                    for pv in eval_data[\"pvs\"]:\n",
    "                        try: # some evals don't have \"cp\" because mate is present. \n",
    "                            cp = pv[\"cp\"]\n",
    "                        except:\n",
    "                            cp = pv[\"mate\"] # treating mate just like cp; if mate is present, it just means the position is extremely winning for one side\n",
    "                        cp = max(-15, min(cp, 15))  # Clamp the value of \"cp\" between -15 and 15\n",
    "                        total_cp += cp\n",
    "                        num_eval += 1\n",
    "                        \n",
    "                if num_eval > 0: #guard clause to avoid division by zero errors\n",
    "                    average_cp = total_cp / num_eval\n",
    "                else:\n",
    "                    average_cp = 0\n",
    "\n",
    "                # Insert average cp for the position into the database\n",
    "                cursor.execute('''INSERT INTO evaluations_avg (fen, average_cp)\n",
    "                                VALUES (?, ?)''', (fen, average_cp))\n",
    "\n",
    "    # Commit changes and close connection\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if it worked as intended\n",
    "conn_ = sqlite3.connect('../assets/data/evaluations_avg.sqlite')\n",
    "df_sample = pd.read_sql(\"\"\" SELECT *\n",
    "                    FROM evaluations_avg\n",
    "                    LIMIT 10000\n",
    "                 \"\"\", conn_)\n",
    "\n",
    "df_sample.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "I was hoping that we would get an evaluation that's closer to the truth: the first position in the df is the starting position\n",
    "and it's evaluated at +3.11. In reality, it should evaluate to 0.5. Inspecting the data, it's far off the actual eval when I paste\n",
    "the FEN in an online engine.\n",
    "\n",
    "My hope is that with this large dataset to train on, we can make sensible decisions regardless of the fact that a portion of the data is off.\n",
    "While +3 is wrong for the starting position, it is true that it's generally better for white. I'm curious to see if that is enough to make a sensible decision on which move to make.\n",
    "\n",
    "We'll call this V1. If it doesn't work, there is always a database of games available on lichess that contains the exact evaluation from Stockfish. However this database is huge and a bit above my current skillset.\n",
    "\n",
    "## Translating FEN notation to neuron-speak\n",
    "BitBoards are 64-bits numbers that represent the presence or absence of a given piece. Since there are 12 different pieces (6 for each colors), we need at least 12 bitboards to represent a complete board state. Let's try the direct translation of the 12 bitboards together to an integer and use that as input to the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the chess module has a function that returns an int representation of the bitboards for a given position\n",
    "\n",
    "board = chess.Board(chess.STARTING_FEN)\n",
    "board.occupied\n",
    "\n",
    "# I am not convinced that this is the correct integer representation, but as long as it's consistent, the\n",
    "# model will learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the entire db into memory. Luckily it takes only 320 mb\n",
    "\n",
    "df_full = pd.read_sql(\"\"\" SELECT *\n",
    "                    FROM evaluations_avg\n",
    "                        \"\"\", conn_)\n",
    "df_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column with the int representation of the FEN position\n",
    "\n",
    "def fen_to_bitboard_int(fen: str) -> int:\n",
    "    \"\"\" return an int representation of the fen, returns None if there's an error\"\"\"\n",
    "    try:\n",
    "        return chess.Board(fen).occupied\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "df_full[\"bitboard_int\"] = df_full[\"fen\"].apply(fen_to_bitboard_int)\n",
    "df_full.dropna(inplace=True) # drop some errors\n",
    "df_full.drop(columns=\"fen\", inplace=True) # we don't need the fen anymore\n",
    "\n",
    "# Save the DataFrame as a new SQLite database\n",
    "temp_conn = sqlite3.connect('../assets/data/train_data_v1.sqlite')\n",
    "df_full.to_sql('train_data_v1', temp_conn, if_exists='replace', index=False)\n",
    "temp_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "I'm doing this with Tensorflow using Keras interface. I want to go up to 64 neurons, 1 for every square. It's only looking at 1 feature and has 1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training df from sqlite\n",
    "temp_conn = sqlite3.connect('../assets/data/train_data_v1.sqlite')\n",
    "df_train = pd.read_sql(\"\"\" SELECT *\n",
    "                    FROM train_data_v1\n",
    "                        \"\"\", temp_conn)\n",
    "temp_conn.close()\n",
    "X = df_train[\"bitboard_int\"]\n",
    "y = df_train[\"average_cp\"]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scaling between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X = np.array(X)\n",
    "X = X.reshape(-1, 1)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "model1 = keras.Sequential(\n",
    "    [\n",
    "        # Input layer -> 1 integer\n",
    "        keras.Input(shape=(1,)),\n",
    "        # Hidden layers\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64), #inserting 1 batch norm\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        # output layer -> 1 raw integer (prediction)\n",
    "        layers.Dense(1, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model1.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", metrics=[\"root_mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first training session, batch size of 100 and 1 epoch seems like a good start\n",
    "model1.fit(X, y, batch_size=50, epochs=1, validation_split=0.1)\n",
    "\n",
    "# Testing notes:\n",
    "# did 2 more epochs, it's not learning anymore, I'll run 200 batch size and see if it improves\n",
    "# after 3 epochs of 200 batch size, it's barely improving. increasing to 500 batch size.\n",
    "# 3 more epochs, it's actually getting worse. Let's try a different architecture and start over\n",
    "\n",
    "# doing 1, 32, 64, 64, 64, 32, 1 and running 3 epochs with a starting batch size of 10. EDIT: it takes 10 min/epoch, ran only 1\n",
    "# it's already better with 1 epoch. I'll increase the batch size to 50 so I don't die of old age.\n",
    "# added a batchnormalization to try to improve accuracy\n",
    "# scaled the data between 0 and 1, immediate increase!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prediction(fen:str) -> int:\n",
    "    \"\"\"Takes a fen a returns a prediction int made by the model\"\"\"\n",
    "    x = fen_to_bitboard_int(fen)\n",
    "    # Reshape the array to have shape (1,)\n",
    "    x_array = np.array([x])\n",
    "    x_reshaped = x_array.reshape(1, -1)\n",
    "    x_transformed = scaler.transform(x_reshaped)\n",
    "    return model1.predict(x_transformed)[0][0]\n",
    "\n",
    "print(get_prediction(\"rnbqr1k1/ppp2ppp/8/4N3/1P1bQ3/P7/5PPP/R1B1KB1R w KQ - 0 13\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the way I translate the FEN\n",
    "To no one's surprise, the approach of transforming the position to an integer makes it lose a lot of its meaning, and the result integer is so enormous that it needs to get scaled back down, which blurs it even further.\n",
    "On a range of -15 to 15, the model was getting at best an average error of 13.3. Horrible.\n",
    "\n",
    "Instead, I will convert the FEN notation into a single 1D array (773) to represent the piece positions and add a few numbers for game state details. Hopefully this will help the model paint a clearer picture of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16,   4,   0,  32,   0, 128,   4,   2,   0,  64,   1,   0, 128,\n",
       "         8,   0, 128,   8,   0, 128,   8,   0, 128,   8,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   2,   0,  32,   2,   0,  32,\n",
       "         2,   0,  32,   2,   0,  32,   0,  64,  16,   0, 128,   2,   0,\n",
       "        16,   8,   1,   0,   4, 248], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fen2bitboard(fen: str, to_bits: bool=False) -> np.array:\n",
    "    \"\"\"\n",
    "    Returns bitboard [np 1D array(773)] from fen\n",
    "    \"\"\"\n",
    "    # each square is assigned 12 bits to represent each piece, that's what the mapping is for\n",
    "    mapping = {\n",
    "                \"p\": 0,\n",
    "                \"n\": 1,\n",
    "                \"b\": 2,\n",
    "                \"r\": 3,\n",
    "                \"q\": 4,\n",
    "                \"k\": 5,\n",
    "                \"P\": 6,\n",
    "                \"N\": 7,\n",
    "                \"B\": 8,\n",
    "                \"R\": 9,\n",
    "                \"Q\": 10,\n",
    "                \"K\": 11\n",
    "                }\n",
    "    \n",
    "    # initialize the array with zeros\n",
    "    bitboard = np.zeros(773, dtype=int)\n",
    "    currIndex = 0\n",
    "    \n",
    "    try:\n",
    "        position, turn, castling, _, _, _ = fen.split(\" \") # keep only useful data\n",
    "    except:\n",
    "        position, turn, castling, _ = fen.split(\" \")\n",
    "    \n",
    "    for ch in position:\n",
    "        if ch == \"/\": # \"/\" represent rows, simply ignore that\n",
    "            continue\n",
    "        elif ch.isdigit(): # a digit means an empty space, skip ahead that many indexes\n",
    "            currIndex += int(ch) * 12 # multiply by 12 because there are 12 bits used for each square\n",
    "        else:\n",
    "            bitboard[currIndex + mapping[ch]] = 1 # set the correct bit to 1\n",
    "            currIndex += 12 # get to next bit\n",
    "    \n",
    "    # add details about the game state\n",
    "    bitboard[768] = 1 if turn == \"w\" else 0\n",
    "    bitboard[769] = 1 if \"K\" in castling else 0\n",
    "    bitboard[770] = 1 if \"Q\" in castling else 0\n",
    "    bitboard[771] = 1 if \"k\" in castling else 0\n",
    "    bitboard[772] = 1 if \"q\" in castling else 0\n",
    "    \n",
    "    if to_bits:\n",
    "        return np.packbits(bitboard)\n",
    "    return bitboard\n",
    "\n",
    "fen2bitboard(chess.STARTING_FEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">350</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">270,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">63,180</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,290</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">360</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,824</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m350\u001b[0m)            │       \u001b[38;5;34m270,900\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)            │        \u001b[38;5;34m63,180\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m)             │        \u001b[38;5;34m16,290\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m)             │           \u001b[38;5;34m360\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m5,824\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">358,667</span> (1.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m358,667\u001b[0m (1.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">358,487</span> (1.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m358,487\u001b[0m (1.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">180</span> (720.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m180\u001b[0m (720.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's adjust our model for this\n",
    "\n",
    "model2 = keras.Sequential(\n",
    "    [\n",
    "        # Input layer -> 1 integer\n",
    "        keras.Input(shape=(773,)),\n",
    "        # Hidden layers\n",
    "        layers.Dense(350, activation=\"relu\"),\n",
    "        layers.Dense(180, activation=\"relu\"),\n",
    "        layers.Dense(90), #inserting 1 batch norm\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        # output layer -> 1 raw integer (prediction)\n",
    "        layers.Dense(1, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model2.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", metrics=[\"root_mean_squared_error\"])\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling large data manipulation\n",
    "I started dipping my toes into bigger data from here. An array (773) x 21M rows gets pretty large, so it's impossible for my computer to handle all the transformations from FEN to array in one go and save it like I did above. The way I dealt with this is by selecting and applying the transformation 1000 rows at a time. The result is then transformed to a tensor dataset and used for training. I tried saving this dataset (apparently tensforflow is separating it in multiple shards automatically) but it just crashed my kernel entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a connection\n",
    "conn = sqlite3.connect('../assets/data/evaluations_avg.sqlite')\n",
    "\n",
    "# Fetch the number of rows in the table\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT COUNT(*) FROM evaluations_avg\")\n",
    "num_rows = cursor.fetchone()[0]\n",
    "cursor.close()\n",
    "\n",
    "# Initialize empty arrays\n",
    "X_data = np.empty((num_rows, 773), dtype=np.int16)\n",
    "y_data = np.empty((num_rows,), dtype=np.float32)\n",
    "\n",
    "# Fetch data from the SQLite database in batches\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM evaluations_avg\")\n",
    "batch_size = 1000\n",
    "start_idx = 0\n",
    "while True:\n",
    "    rows = cursor.fetchmany(batch_size)\n",
    "    if not rows:\n",
    "        break\n",
    "    for row in rows:\n",
    "        X_data[start_idx] = fen2bitboard(row[0])\n",
    "        y_data[start_idx] = row[1]\n",
    "        start_idx += 1\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(773,), dtype=tf.int16, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create TensorFlow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
    "dataset.element_spec\n",
    "\n",
    "# making train and test data\n",
    "test_dataset = dataset.take(10000) # takes the first 10000 of the data for\n",
    "train_dataset = dataset.skip(10000) # takes the remaining for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell and the following for extra training\n",
    "model2 = keras.models.load_model(\"first_model_keep_training.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1395s\u001b[0m 1ms/step - loss: 4.5089 - root_mean_squared_error: 8.0830 - val_loss: 5.6494 - val_root_mean_squared_error: 8.9893\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1400s\u001b[0m 1ms/step - loss: 4.4555 - root_mean_squared_error: 8.0211 - val_loss: 5.8084 - val_root_mean_squared_error: 9.0748\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1422s\u001b[0m 1ms/step - loss: 4.4323 - root_mean_squared_error: 7.9969 - val_loss: 5.6499 - val_root_mean_squared_error: 8.9295\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1429s\u001b[0m 1ms/step - loss: 4.4154 - root_mean_squared_error: 7.9812 - val_loss: 5.6932 - val_root_mean_squared_error: 9.0842\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1449s\u001b[0m 1ms/step - loss: 4.3997 - root_mean_squared_error: 7.9620 - val_loss: 5.7224 - val_root_mean_squared_error: 9.1301\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1461s\u001b[0m 1ms/step - loss: 4.3882 - root_mean_squared_error: 7.9519 - val_loss: 5.6152 - val_root_mean_squared_error: 8.9583\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1468s\u001b[0m 1ms/step - loss: 4.3747 - root_mean_squared_error: 7.9345 - val_loss: 5.5796 - val_root_mean_squared_error: 8.8829\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1472s\u001b[0m 1ms/step - loss: 4.3657 - root_mean_squared_error: 7.9256 - val_loss: 5.6294 - val_root_mean_squared_error: 8.9222\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1499s\u001b[0m 1ms/step - loss: 4.3558 - root_mean_squared_error: 7.9161 - val_loss: 5.6583 - val_root_mean_squared_error: 9.0238\n",
      "\u001b[1m1049464/1049464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1491s\u001b[0m 1ms/step - loss: 4.3462 - root_mean_squared_error: 7.9070 - val_loss: 5.5950 - val_root_mean_squared_error: 8.8748\n"
     ]
    }
   ],
   "source": [
    "# save after every epoch to make sure we don't lose progress\n",
    "for _ in range(10):\n",
    "    model2.fit(train_dataset.batch(20), epochs=1, validation_data=test_dataset.batch(100))\n",
    "    model2.save(\"first_model_keep_training.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I'm thrilled to announce the successful training of this model! Each epoch takes approximately 45 minutes to complete, and I've been consistently running it overnight, saving the generated models along the way. The progress has been notable, with the evaluation output now demonstrating the capability to make sensible chess decisions at a beginner level.\n",
    "\n",
    "While the current training method has yielded results, there's room for improvement. My goal is to enhance the design by implementing a process that involves randomly sampling from a larger database, applying transformations, and feeding the data into the model for training. This approach would allow me to train on a more extensive dataset and utilize the actual Stockfish evaluation from a given position, rather than relying on the average evaluation from subsequent moves. Essentially, I aim to utilize the functionality of the from_generator function.\n",
    "\n",
    "Overall, I'm pleased with the outcome of this project. It has provided me with valuable experience in utilizing SQLite and handling large databases, as well as the opportunity to develop a competent chess engine and enhance my OOP skills. I look forward to revisiting this project in the future to further refine and improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a list of things I tried that didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(773,), dtype=tf.int16, name=None),\n",
       " TensorSpec(shape=(1,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying a generator -> gets a conflict with SQL\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# open a connection\n",
    "conn = sqlite3.connect('../assets/data/evaluations_avg.sqlite')\n",
    "\n",
    "def data_generator():\n",
    "        \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM evaluations_avg\")\n",
    "    batch_size = 1000\n",
    "    while True:\n",
    "        rows = cursor.fetchmany(batch_size)\n",
    "        if not rows:\n",
    "            break\n",
    "        X_batch = [fen2bitboard(row[0]) for row in rows]  # Feature column\n",
    "        y_batch = [row[1] for row in rows]  # Label column\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "# # close when done\n",
    "# conn.close()\n",
    "# cursor.close()\n",
    "\n",
    "\n",
    "# create a tensor dataset\n",
    "dataset = tf.data.Dataset.from_generator(generator = data_generator,\n",
    "                                        output_signature= (\n",
    "                                        tf.TensorSpec(shape=(773,), dtype=tf.int16), # bitboard\n",
    "                                        tf.TensorSpec(shape=(1), dtype=tf.float32)  # average_cp\n",
    "                                    )\n",
    "                                    )\n",
    "\n",
    "\n",
    "dataset.element_spec\n",
    "# it doesn't work with SQL query, I'm running into graph errors and asks me to use only 1 thread for the sql queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found out tensorflow has a built-in method for sql db...\n",
    "# doesn't work because the saved data is in bytestrings and needs translation. It's not an improvement.\n",
    "\n",
    "dataset = tf.data.experimental.SqlDataset(\"sqlite\", \"../assets/data/train_data_v2.sqlite\", \n",
    "                                          \"SELECT average_cp, bitboard FROM train_data_v2\", \n",
    "                                          (tf.float64, tf.string))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
